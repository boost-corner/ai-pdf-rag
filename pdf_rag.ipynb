{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5b48f85ecae031",
   "metadata": {},
   "source": [
    "# AI That Reads and Understands PDFs with LangGraph, OpenAI, and PyPDF\n",
    "\n",
    "Easily build an AI that reads and understands your PDF documents using LangGraph, OpenAI, and PyPDF. Perfect for developers, researchers, and teams who need to extract insights and answer questions from documents with minimal effort.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Input Parameters:** PDF document(s), user question  \n",
    "- **Output Structure:** Extracted context, summarized insights, and direct answers to questions based on document content  \n",
    "- **Technologies:** LangGraph, OpenAI API, PyPDF  \n",
    "- **Use Cases:** Document search and QA, legal and research document analysis, internal knowledge base exploration, and intelligent PDF assistants\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Jupyter Notebook\n",
    "\n",
    "This and other tutorials are perhaps most conveniently run in a [Jupyter notebooks](https://jupyter.org/). Going through guides in an interactive environment is a great way to better understand them. See [here](https://jupyter.org/install) for instructions on how to install.\n",
    "\n",
    "### Installation\n",
    "\n",
    "This tutorial requires these langchain dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8304de2d19e2e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade langchain-community langgraph langchain-core \"langchain[openai]\" typing_extensions langchain-text-splitters pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ee08943daeb78c",
   "metadata": {},
   "source": [
    "### LangSmith\n",
    "\n",
    "Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\n",
    "As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\n",
    "The best way to do this is with [LangSmith](https://smith.langchain.com).\n",
    "\n",
    "After you sign up at the link above, make sure to set your environment variables to start logging traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445d29ed75e71723",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T00:03:55.984161Z",
     "start_time": "2025-03-27T00:03:54.423534Z"
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c966f8d40cb03be",
   "metadata": {},
   "source": [
    "## Step 1: Load, Chunk, Embed, and Index the PDF\n",
    "\n",
    "In this step, we will:\n",
    "\n",
    "1. Load a PDF file from a [URL](https://arxiv.org/pdf/2312.10997) \n",
    "2. Split it into smaller chunks\n",
    "3. Generate semantic embeddings using OpenAI's `text-embedding-3-large`\n",
    "4. Store the embeddings in an in-memory vector store for fast querying\n",
    "\n",
    "### üîê OpenAI API Key Setup\n",
    "\n",
    "To generate embeddings using OpenAI‚Äôs `text-embedding-3-large` model, you‚Äôll need an `OPENAI_API_KEY`.\n",
    "\n",
    "Follow these steps to get your API key:\n",
    "\n",
    "1. Go to [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)\n",
    "2. Log into your OpenAI account (or sign up if you don‚Äôt have one)\n",
    "3. Click **\"Create new secret key\"**\n",
    "4. Copy the generated API key and store it securely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f1459b2f16bdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T00:05:28.721238Z",
     "start_time": "2025-03-27T00:05:27.055715Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3fcbf6e0cd9667",
   "metadata": {},
   "source": [
    "To start, we'll load a PDF document from a public URL and prepare it for processing.  \n",
    "This involves three key steps:\n",
    "\n",
    "1. Initializing the OpenAI embedding model (`text-embedding-3-large`)\n",
    "2. Setting up an in-memory vector store to hold our document chunks\n",
    "3. Loading the PDF content using LangChain's `PyPDFLoader`\n",
    "\n",
    "This will give us the raw text we need before splitting and embedding it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f00b20618da5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T00:05:34.778367Z",
     "start_time": "2025-03-27T00:05:32.532029Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF file URL\n",
    "file_path = \"https://arxiv.org/pdf/2312.10997\"\n",
    "\n",
    "# Initialize embedding model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Initialize in-memory vector store\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "# Load PDF\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9d3466e0090d60",
   "metadata": {},
   "source": [
    "### üìö What Are Chunks?\n",
    "\n",
    "Chunks are small, coherent pieces of text extracted from the original document.  \n",
    "Since large language models (LLMs) like GPT have a limited context window, we can't process the entire document at once.\n",
    "\n",
    "By splitting the content into overlapping segments (chunks), we enable:\n",
    "\n",
    "- Efficient semantic search\n",
    "- Better relevance during question answering\n",
    "- Reduced risk of token overflow errors\n",
    "\n",
    "In this tutorial, we use a chunk size of **1000 characters** with **200-character overlap** to preserve context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c0c4be61d6e83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T00:05:43.848177Z",
     "start_time": "2025-03-27T00:05:43.833077Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split the text into chunks of ~1000 characters with 200 characters overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "text_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Let's inspect the first chunk\n",
    "text_splits[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48495aea27d49b57",
   "metadata": {},
   "source": [
    "### üß† What Are Vectors?\n",
    "\n",
    "Vectors are high-dimensional numerical representations of text.  \n",
    "They are created by passing a text chunk through an **embedding model**, such as OpenAI's `text-embedding-3-large`.\n",
    "\n",
    "These vectors capture the **semantic meaning** of the text. Texts with similar meaning will have embeddings (vectors) that are close to each other in vector space.\n",
    "\n",
    "We use these vectors to:\n",
    "\n",
    "- Search for similar content\n",
    "- Match user questions to relevant document chunks\n",
    "- Enable context-aware AI responses\n",
    "\n",
    "Now that we have the document split into chunks, let's generate embeddings for each chunk and store them in our vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c031d919ce5b413",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T00:05:57.486636Z",
     "start_time": "2025-03-27T00:05:54.265489Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add all chunks to the vector store (embedding happens under the hood)\n",
    "vector_store.add_documents(documents=text_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52232d2e744d3d73",
   "metadata": {},
   "source": [
    "## Step 2: Retrieve & Generate\n",
    "\n",
    "Now that we‚Äôve embedded and indexed the document, we can use it to answer user questions.\n",
    "\n",
    "This step involves two parts:\n",
    "\n",
    "1. **Retrieve** the most relevant document chunks based on a user query using semantic similarity.\n",
    "2. **Generate** a concise, context-aware response using a language model (LLM) and a custom prompt.\n",
    "\n",
    "We will use OpenAI‚Äôs GPT model to generate answers grounded in the retrieved document context.\n",
    "\n",
    "### üß† Define the Prompt Template for Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "The prompt template will instruct the LLM to use only the retrieved context when answering the user‚Äôs question.  \n",
    "This prevents hallucinations and keeps answers grounded in real data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fa25de27ebc47e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T00:06:08.188466Z",
     "start_time": "2025-03-27T00:06:08.181362Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "rag_prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878b1f67d41ead75",
   "metadata": {},
   "source": [
    "### ‚ùì Enter a Question Related to the [PDF](https://arxiv.org/pdf/2312.10997)\n",
    "\n",
    "Here are some example questions you can ask based on the content of the paper:\n",
    "\n",
    "- *What problems does RAG solve in large language models?*\n",
    "- *What are the three paradigms of RAG described in the paper?*\n",
    "- *How does RAG mitigate hallucination in LLMs?*\n",
    "- *What is Modular RAG and how is it different from Naive RAG?*\n",
    "- *What challenges in RAG research are mentioned in the paper?*\n",
    "\n",
    "Feel free to enter your own question related to the paper.\n",
    "\n",
    "\n",
    "üîç **What happens after you enter a question?**\n",
    "\n",
    "When you type your question and run the next cell:\n",
    "\n",
    "1. The system will **search the indexed document chunks** for the most semantically similar content.\n",
    "2. It uses **vector similarity** (cosine distance) to find the most relevant passages in the PDF.\n",
    "3. These passages are stored in memory and used as **context for the language model** in the next step.\n",
    "\n",
    "This allows the model to give precise, grounded answers instead of guessing or hallucinating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2554607b59e1463b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T00:06:19.961238Z",
     "start_time": "2025-03-27T00:06:11.996920Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let the user enter a custom question about the RAG survey paper\n",
    "user_question = input(\"Question: \")\n",
    "\n",
    "# Perform semantic similarity search\n",
    "retrieved_docs = vector_store.similarity_search(user_question)\n",
    "\n",
    "# Store in simulated state\n",
    "state = {\n",
    "    \"question\": user_question,\n",
    "    \"docs\": retrieved_docs\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4d821040173fb9",
   "metadata": {},
   "source": [
    "### üìÑ Prepare the Context and Generate an Answer\n",
    "\n",
    "Now that we‚Äôve retrieved the most relevant document chunks, we‚Äôll format them into a single text block.  \n",
    "We then pass this context along with the user‚Äôs question into our prompt template.\n",
    "\n",
    "Finally, we invoke the language model (GPT-4) to generate a concise answer based strictly on the provided context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed096061e4ce203",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T00:06:29.763777Z",
     "start_time": "2025-03-27T00:06:24.966530Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Format the context into a string\n",
    "context = \"\\n\\n\".join(doc.page_content for doc in state[\"docs\"])\n",
    "\n",
    "# Fill the prompt template\n",
    "question_prompt = rag_prompt_template.invoke({\n",
    "    \"question\": state[\"question\"],\n",
    "    \"context\": context\n",
    "})\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Get the response\n",
    "response = llm.invoke(question_prompt)\n",
    "\n",
    "# Show the answer\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7fb4f3c3009571",
   "metadata": {},
   "source": [
    "## Step 3: Build a RAG Workflow Using LangGraph\n",
    "\n",
    "In this step, we'll use [LangGraph](https://github.com/langchain-ai/langgraph) to orchestrate the entire **Retrieval-Augmented Generation (RAG)** pipeline as a stateful workflow.\n",
    "\n",
    "LangGraph allows us to define the steps in a computation as nodes in a graph, making the pipeline modular and reusable.\n",
    "\n",
    "Our workflow will:\n",
    "\n",
    "1. Accept a user question\n",
    "2. Retrieve the most relevant chunks from the vector store\n",
    "3. Use a language model to generate a concise, grounded answer\n",
    "\n",
    "### Define the Application State\n",
    "\n",
    "We'll start by defining the data structure (state) that will be passed between nodes in the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d682460c812404f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T00:06:47.029195Z",
     "start_time": "2025-03-27T00:06:47.024609Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class RagState(TypedDict):\n",
    "    question: str\n",
    "    docs: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c93b0e7f76a17c0",
   "metadata": {},
   "source": [
    "### üîç Define the Retrieval Function\n",
    "\n",
    "This function receives the question, performs a similarity search using our in-memory vector store,  \n",
    "and returns the top matching documents. These will be passed to the next step in the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adea2cf40023acfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T00:06:49.010376Z",
     "start_time": "2025-03-27T00:06:49.004719Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve(state: RagState):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"docs\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cde2152986232f0",
   "metadata": {},
   "source": [
    "### üß† Define the Answer Generation Function\n",
    "\n",
    "This function takes the retrieved documents and the original question,  \n",
    "formats them using our prompt template, and calls the LLM to generate a final answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f34515eed85d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T00:06:50.985077Z",
     "start_time": "2025-03-27T00:06:50.982078Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate(state: RagState):\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in state[\"docs\"])\n",
    "    question_prompt = rag_prompt_template.invoke({\n",
    "        \"question\": state[\"question\"],\n",
    "        \"context\": context\n",
    "    })\n",
    "    response = llm.invoke(question_prompt)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2254a17c0bd084b",
   "metadata": {},
   "source": [
    "### üîÑ Create the Workflow Using LangGraph\n",
    "\n",
    "Now we'll connect the nodes: first retrieval, then generation.  \n",
    "We also specify the entry point and the terminal step using LangGraph's API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff45fee29e9635f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T00:06:53.041075Z",
     "start_time": "2025-03-27T00:06:52.988093Z"
    }
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Create the graph\n",
    "workflow = StateGraph(RagState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "# Define flow\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile the graph into an executable app\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed57f91b9f6b9991",
   "metadata": {},
   "source": [
    "### ‚ñ∂Ô∏è Run the Workflow with a Custom Question\n",
    "\n",
    "You can now run the complete RAG workflow by providing a new question.\n",
    "The system will retrieve context and generate an answer in a fully automated fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b073807c6ad69d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T00:07:12.313446Z",
     "start_time": "2025-03-27T00:06:55.345639Z"
    }
   },
   "outputs": [],
   "source": [
    "question = input(\"Ask a question about the RAG paper: \")\n",
    "\n",
    "result = app.invoke({\"question\": question})\n",
    "\n",
    "print(\"üîç Answer:\\n\", result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
